from collections import OrderedDict
import spacy  # NLP with spaCy https://spacy.io
nlp = spacy.load('en')  # will take some time to load

# Source from https://github.com/kengz/spacy-nlp/blob/master/src/py/nlp.py
# Useful properties, summary of the docs from https://spacy.io

# class Doc
# properties: text, vector, vector_norm, ents, noun_chunks, sents
# method: similarity
# NER specs https://spacy.io/docs#annotation-ner
# doc tokenization will preserve meaningful units together

# class Token
# token.doc -> parent sequence
# string features: text, lemma, lower, shape
# boolean flags: https://spacy.io/docs#token-booleanflags
# POS: pos_, tag_
# tree: https://spacy.io/docs#token-navigating
# ner: ent_type, ent_iob

# class Span
# span.doc -> parent sequence
# vector, vector_norm
# string features: text, lemma
# methods: similarity
# syntactic parse: use root, lefts, rights, subtree
# https://spacy.io/docs#span-navigativing-parse


# !more to implement:
# also filter to prepare for tree
# syntactic parse tree https://spacy.io/docs#span-navigativing-parse
# word2vec, numpy array
# similarity https://spacy.io/docs#examples-word-vectors
# https://spacy.io/docs#span-similarity

# https://github.com/spacy-io/sense2vec/
# tuts https://spacy.io/docs#tutorials
# custom NER and intent arg parsing eg
# https://github.com/spacy-io/spaCy/issues/217


# Helper methods
##########################################

def merge_ents(doc):
    '''Helper: merge adjacent entities into single tokens; modifies the doc.'''
    for ent in doc.ents:
        ent.merge()
    return doc


def format_POS(token, light=False, flat=False):
    '''helper: form the POS output for a token'''
    subtree = {
        "word": token.text,
        "lemma": token.lemma_,  # trigger
        "NE": token.ent_type_,  # trigger
        "POS_fine": token.tag_,
        "POS_coarse": token.pos_,
        "arc": token.dep_,
        "modifiers": [],
    }
    if light:
        del subtree["lemma"]
        del subtree["NE"]
    if flat:
        del subtree["arc"]
        del subtree["modifiers"]
    return subtree


def POS_tree_(root, light=False):
    '''
    Helper: generate a POS tree for a root token.
    The doc must have merge_ents(doc) ran on it.
    '''
    subtree = format_POS(root, light=light)
    for c in root.children:
        subtree["modifiers"].append(POS_tree_(c))
    return subtree


def parse_tree(doc, light=False):
    '''generate the POS tree for all sentences in a doc'''
    merge_ents(doc)  # merge the entities into single tokens first
    return [POS_tree_(sent.root, light=light) for sent in doc.sents]


def parse_list(doc, light=False):
    '''tag the doc first by NER (merged as tokens) then
    POS. Can be seen as the flat version of parse_tree'''
    merge_ents(doc)  # merge the entities into single tokens first
    return [format_POS(token, light=light, flat=True) for token in doc]

# s = "find me flights from New York to London next month"
# doc = nlp(s)
# parse_list(doc)


# Primary methods
##########################################

def parse_sentence(sentence):
    '''
    Main method: parse an input sentence and return the nlp properties.
    '''
    doc = nlp(sentence)
    reply = {
        "text": doc.text,
        "len": len(doc),
        "tokens": [token.text for token in doc],
        "noun_phrases": [token.text for token in doc.noun_chunks],
        "parse_tree": parse_tree(doc),
        "parse_list": parse_list(doc),
    }
    return reply

# res = parse_sentence("find me flights from New York to London next month.")


def parse(text_input):
    '''
    parse for multi-sentences; split and apply parse in a list.
    '''
    doc = nlp(text_input, disable=['tagger', 'ner', 'textcat'])
    return [parse_sentence(sent.text) for sent in doc.sents]

# print(parse("Bob brought the pizza to Alice. I saw the man with glasses."))
